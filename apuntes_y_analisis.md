# 游늯 APUNTES Y AN츼LISIS

## 游댳 UNIDAD 2: FUENTES DE INFORMACI칍N

### `calculoEntropia(probabilidades)` (H)
* **Procedimiento:** Se obtiene calculando la esperanza matem치tica de la informaci칩n propia de cada s칤mbolo. Es la sumatoria de $p_i \cdot \log_r(1/p_i)$ para cada probabilidad en la distribuci칩n.
* **An치lisis:**
    * Representa la **incertidumbre promedio** de la fuente o la cantidad de informaci칩n promedio que aporta cada s칤mbolo.
    * **Valor M치ximo:** Se alcanza cuando la fuente es **equiprobable** (m치xima incertidumbre).
    * **Valor M칤nimo (0):** Ocurre si la fuente es determin칤stica (un s칤mbolo tiene probabilidad 1).

---

## 游댳 UNIDAD 4: CODIFICACI칍N Y COMPRESI칍N

### `cumpleShannon(probabilidades, codigos, N)`
* **Procedimiento:** Se calcula la entrop칤a de la fuente $H(S)$ y la longitud media del c칩digo $L$ (suma de longitudes ponderadas por probabilidad). Se verifica si $L$ satisface el Primer Teorema: $H(S) \le L < H(S) + 1/N$.
* **An치lisis:**
    * Si devuelve **True**, el c칩digo es eficiente y te칩ricamente posible.
    * Si $L < H(S)$, el c칩digo violar칤a el teorema (ser칤a imposible decodificar sin p칠rdida).
    * Al aumentar la extensi칩n $N$, la longitud media por s칤mbolo se acerca asint칩ticamente a la entrop칤a.

### `algoritmoHuffman(probabilidades)`
* **Procedimiento:**
    1. Se inicia con nodos hoja, uno para cada s칤mbolo y su probabilidad.
    2. Se seleccionan los dos nodos con menor probabilidad.
    3. Se combinan en un nuevo nodo padre (suma de probabilidades). Se asigna '0' a uno y '1' al otro.
    4. Se repite hasta que queda un solo nodo ra칤z.
    5. El c칩digo es el camino desde la ra칤z hasta la hoja.
* **An치lisis:**
    * Genera un c칩digo **칩ptimo** (de longitud media m칤nima) para una fuente dada.
    * Siempre es **instant치neo** (prefijo).
    * Asigna palabras cortas a s칤mbolos probables y largas a s칤mbolos improbables.

### `algoritmoShannonFano(probabilidades)`
* **Procedimiento:**
    1. Se ordenan las probabilidades de mayor a menor.
    2. **Recursividad:** Se divide la lista en dos subgrupos cuyas sumas de probabilidades sean lo m치s parecidas posible.
    3. Se asigna '0' al primer grupo y '1' al segundo.
    4. Se repite el proceso recursivamente para cada subgrupo hasta llegar a probabilidades individuales.
* **An치lisis:**
    * Es un c칩digo eficiente, pero **no siempre es 칩ptimo** (Huffman es igual o mejor).
    * Es **instant치neo** (prefijo).
    * La divisi칩n recursiva busca equilibrar el 치rbol de c칩digos.

### `calculoRendimiento` ($\eta$) y `calculoRedundancia` ($R$)
* **Procedimiento:**
    * **Rendimiento:** Relaci칩n entre la entrop칤a y la longitud media ($\eta = H(S) / L$).
    * **Redundancia:** El complemento del rendimiento ($1 - \eta$).
* **An치lisis:**
    * **Rendimiento:** Indica qu칠 porcentaje de los bits transmitidos lleva informaci칩n real. Un valor cercano a 1 indica alta eficiencia (como en Huffman).
    * **Redundancia:** Indica el porcentaje de capacidad del canal desperdiciada o usada para protecci칩n/estructura.

### `calcularTasaCompresion(original, comprimido)`
* **Procedimiento:** Se divide el tama침o del mensaje original por el tama침o del mensaje codificado ($N = \text{Original} / \text{Comprimido}$).
* **An치lisis:**
    * **Resultado > 1:** Compresi칩n efectiva (ej: 1.5 significa que el original era 1.5 veces m치s grande).
    * **Resultado < 1:** Expansi칩n de datos (el algoritmo fue ineficiente para esa fuente).

### `comprimirRLC(mensaje)` (Run Length Coding)
* **Procedimiento:** Se recorre la cadena contando "rachas" consecutivas del mismo car치cter. Se reemplaza la secuencia por el par `(car치cter, cantidad)`.
* **An치lisis:**
    * Este m칠todo solo es eficiente en fuentes con **baja entrop칤a** (alta redundancia, muchos s칤mbolos repetidos).
    * En fuentes aleatorias, suele aumentar el tama침o del archivo (tasa < 1).

### `calculoHamming(codigos)` (Distancia $d$)
* **Procedimiento:** Se comparan todos los pares posibles de palabras c칩digo y se cuenta en cu치ntos bits difieren (XOR). La distancia $d$ es el **m칤nimo** de esas diferencias.
* **An치lisis:**
    * Determina la capacidad de correcci칩n del c칩digo.
    * **$d=1$:** No detecta errores.
    * **$d=2$:** Detecta 1 error, corrige 0.
    * **$d=3$:** Detecta 2 errores, corrige 1.

### `genBytearrayMatrizParidad` (Paridad 2D: VRC/LRC)
* **Procedimiento:** Se organiza el mensaje en una matriz de bits. Se a침ade una columna para la paridad de cada fila (longitudinal) y una fila para la paridad de cada columna (vertical/cruzada).
* **An치lisis:**
    * Permite **detectar** m칰ltiples errores cruzando la informaci칩n de filas y columnas fallidas.
    * Permite **corregir** exactamente 1 bit de error identificando la intersecci칩n de la fila y columna incorrectas.

### `decodificarParidadMatriz(bytes)` (Correcci칩n de Errores)
* **Procedimiento:**
    1. Desempaqueta la secuencia de bytes en una matriz de bits de $(N+1) \times 8$.
    2. Verifica la paridad (par) de cada fila de datos (1 a N) y de cada columna (0 a 7).
    3. Si detecta exactamente **1 fila y 1 columna** con error, identifica la intersecci칩n y "flippea" ese bit para corregirlo.
    4. Si detecta otros patrones de error (m칰ltiples filas/columnas), reporta error incorregible.
* **An치lisis:**
    * Implementa un **C칩digo de Producto** (Paridad 2D).
    * **Poder:** A diferencia de la paridad simple, este m칠todo **s칤 puede corregir** un error (porque las coordenadas fila/columna lo localizan).
    * **L칤mite:** Solo garantiza corregir 1 bit. Si hay errores m칰ltiples (ej: 2 bits en la misma fila), el sistema falla o detecta el error pero no puede corregirlo.

## 游댳 UNIDAD 5: CANALES DE INFORMACI칍N

### `genMatrizCanal` (Matriz Condicional $P(B|A)$)
* **Procedimiento:** Se cuentan las ocurrencias de cada par entrada-salida $(a_i, b_j)$ y se dividen por la cantidad total de veces que apareci칩 la entrada $a_i$.
* **An치lisis:**
    * Describe el comportamiento f칤sico o **ruido** del canal.
    * Cada fila representa una entrada; cada columna una salida.
    * **Propiedad:** La suma de cada fila debe ser siempre 1.

### `calcularProbabilidadesSalida` ($P(B)$)
* **Procedimiento:** Se suman las **columnas** de la matriz de eventos simult치neos $P(A, B)$. F칩rmula: $P(B_j) = \sum_i P(A_i, B_j)$.
* **An치lisis:**
    * Indica la probabilidad de recibir cada s칤mbolo de salida, independientemente de qu칠 se envi칩.
    * En canales con ruido, la distribuci칩n de salida suele ser m치s "plana" (m치s entrop칤a) que la de entrada debido a la dispersi칩n.

### `calcularMatrizPosteriori` ($P(A|B)$)
* **Procedimiento:** Se aplica el **Teorema de Bayes**: $P(A_i|B_j) = \frac{P(A_i, B_j)}{P(B_j)}$. Se divide cada columna de la matriz simult치nea por la probabilidad de salida correspondiente.
* **An치lisis:**
    * Representa la certeza "hacia atr치s": *Dado que recib칤 $B_j$, 쯤u칠 tan seguro estoy de que enviaron $A_i$?*
    * Si una columna tiene un 1.0 y el resto 0, significa que ese s칤mbolo de salida elimina toda incertidumbre (caracter칤stica de canal sin ruido).

### `calcularMatrizSimultanea` ($P(A, B)$)
* **Procedimiento:** Se aplica la **Regla de la Multiplicaci칩n**: $P(A_i, B_j) = P(B_j|A_i) \cdot P(A_i)$. Se multiplica cada fila de la matriz del canal por su probabilidad a priori correspondiente.
* **An치lisis:**
    * Representa la probabilidad de que ocurra el evento conjunto "se envi칩 $A_i$ **Y** se recibi칩 $B_j$".
    * Es la base para calcular todo lo dem치s ($P(B)$, $P(A|B)$, Entrop칤a Af칤n).
    * La suma de todos los elementos de la matriz debe ser 1.

### `calcularEntropiasPosteriori` (Lista de $H(A|b_j)$)
* **Procedimiento:**
    1. Obtiene la matriz a posteriori $P(A|B)$.
    2. Recorre la matriz por **columnas** (cada columna corresponde a una salida $b_j$).
    3. Calcula la entrop칤a individual de la distribuci칩n de probabilidad de esa columna.
* **An치lisis:**
    * Devuelve una lista de valores. Cada valor representa la **incertidumbre espec칤fica** que queda sobre la entrada cuando se observa esa salida particular.
    * Si un valor de la lista es 0, significa que esa salida espec칤fica es "perfecta": si la recibo, s칠 exactamente qu칠 se envi칩.
    * El promedio ponderado de esta lista (usando $P(B)$) es la Equivocaci칩n $H(A|B)$.

### `calcularEquivocacion` (Ruido: $H(A|B)$)
* **Procedimiento:** Es el promedio ponderado de las entrop칤as de las **columnas** de la matriz a posteriori $P(A|B)$.
    * Relaci칩n: $H(A|B) = H(A,B) - H(B)$.
* **An치lisis:**
    * Mide la **incertidumbre restante** sobre la entrada despu칠s de observar la salida.
    * **$H(A|B) = 0$:** Canal **Sin Ruido**. Observar la salida me dice exactamente qu칠 se envi칩.
    * Si es alta, la salida no me sirve de mucho para adivinar la entrada.

### `calcularPerdida` ($H(B|A)$)
* **Procedimiento:** Es el promedio ponderado de las entrop칤as de las **filas** de la matriz del canal $P(B|A)$.
    * Relaci칩n: $H(B|A) = H(A,B) - H(A)$.
* **An치lisis:**
    * Mide la dispersi칩n o incertidumbre sobre qu칠 saldr치, dado que s칠 qu칠 entr칠.
    * **$H(B|A) = 0$:** Canal **Determinante**. Si env칤o un s칤mbolo, s칠 100% qu칠 va a salir.

### `calcularEntropiaAfin` ($H(A, B)$)
* **Procedimiento:** Se calcula la entrop칤a de la matriz de probabilidades simult치neas $P(A, B)$ tratada como una 칰nica distribuci칩n.
* **An치lisis:**
    * Mide la incertidumbre total del sistema (entrada + salida).
    * Siempre se cumple que $H(A,B) \le H(A) + H(B)$. (Solo es igual si son eventos independientes, o sea, un canal in칰til).

### `calcularInformacionMutua` ($I(A;B)$)
* **Procedimiento:**
    * **Por definici칩n:** Doble sumatoria de $P(A,B) \cdot \log(\frac{P(A,B)}{P(A)P(B)})$.
    * **Por relaciones:** $I(A;B) = H(A) - H(A|B)$ (Reducci칩n de incertidumbre de entrada).
* **An치lisis:**
    * Representa la **informaci칩n 칰til** que logra atravesar el canal.
    * Es sim칠trica: $I(A;B) = I(B;A)$.
    * **Valor m치ximo:** Es la **Capacidad** del canal ($C$).
    * No puede ser negativa. Si es 0, la salida no tiene relaci칩n con la entrada.

## 游댳 UNIDAD 6: CANALES CON RUIDO (CAPACIDAD Y REDUCCI칍N)

### `esCanalSinRuido(matriz)`
* **Procedimiento:** Se verifica que cada **columna** de la matriz tenga exactamente un elemento distinto de cero.
* **An치lisis:**
    * **Sin Ruido (Lossless):** Si s칠 la salida, s칠 la entrada con certeza.
    * La equivocaci칩n $H(A|B) = 0$.
    * La capacidad es $C = \log_2(\text{Nro de Entradas})$.

### `esCanalDeterminante(matriz)`
* **Procedimiento:** Se verifica que cada **fila** de la matriz tenga exactamente un elemento distinto de cero (que, por propiedad de probabilidad, debe ser 1.0).
* **An치lisis:**
    * **Determinante (Deterministic):** Si s칠 la entrada, s칠 la salida con certeza.
    * La p칠rdida $H(B|A) = 0$.
    * La capacidad es $C = \log_2(\text{Nro de Salidas})$.

### `calcularMatrizCompuesta` (Canales en Serie)
* **Procedimiento:** Se realiza la **multiplicaci칩n de matrices**: $P(C|A) = P(B|A) \times P(C|B)$.
* **An치lisis:**
    * Representa la probabilidad de transici칩n total desde la entrada inicial hasta la salida final, sumando todos los caminos intermedios posibles.
    * Al conectar canales en serie, la informaci칩n mutua tiende a disminuir ($I(A;C) \le I(A;B)$) y el ruido a aumentar.

### `verificarColumnasProporcionales` (Reducci칩n Suficiente)
* **Procedimiento:** Se verifica si existe una constante $k$ tal que $Col_1 = k \cdot Col_2$ (o viceversa) para todas las filas de la matriz.
* **An치lisis:**
    * Si dos columnas son proporcionales, aportan la misma informaci칩n relativa sobre la entrada.
    * **Teorema:** Agrupar columnas proporcionales en una sola salida **no** reduce la Informaci칩n Mutua ($I(A;B)$ se mantiene igual). Es una "Reducci칩n Suficiente".
    * Permite simplificar el modelo del canal sin perder informaci칩n.

### `genMatrizReduccion` (Matriz de Transformaci칩n)
* **Procedimiento:** Genera una matriz determinante $P(C|B)$ que mapea las columnas del canal original a un nuevo conjunto de columnas reducidas. Las dos columnas proporcionales se mapean a la misma salida (sum치ndose), y el resto se mapea 1 a 1.
* **An치lisis:**
    * Es el operador matem치tico que realiza la reducci칩n.
    * Al multiplicar el canal original por esta matriz ($P(B|A) \times P(C|B)$), se obtiene el canal reducido $P(C|A)$.

### `realizarReduccionMaxima`
* **Procedimiento:** Busca iterativamente pares de columnas proporcionales y aplica la reducci칩n (usando `genMatrizReduccion` y multiplicaci칩n de matrices) hasta que no se pueden realizar m치s combinaciones.
* **An치lisis:**
    * Transforma el canal en su versi칩n m치s simple posible (m칤nima cantidad de salidas) sin perder Capacidad de Informaci칩n.
    * Si la matriz resultante es cuadrada e identidad (o una permutaci칩n), significa que el canal original era equivalente a un canal sin ruido (aunque tuviera m치s salidas).

### `esCanalUniforme(matriz)`
* **Procedimiento:** Se verifica si todas las filas son **permutaciones** de la primera fila (es decir, tienen los mismos valores de probabilidad, aunque en distinto orden).
* **An치lisis:**
    * En un canal uniforme, la "dispersi칩n" del ruido es igual para cualquier s칤mbolo de entrada.
    * La p칠rdida $H(B|A)$ es constante e igual a la entrop칤a de cualquier fila.
    * La capacidad se calcula con la f칩rmula simplificada: $C = \log_2(\text{Salidas}) - H(\text{fila})$.

### `calcularCapacidadEspecial` (Atajos de Capacidad)
* **Procedimiento:** Clasifica el canal y aplica f칩rmulas simplificadas:
    * **Determinante:** $C = \log_2(\text{Salidas})$. (Porque $H(B|A)=0$).
    * **Sin Ruido:** $C = \log_2(\text{Entradas})$. (Porque $H(A|B)=0$).
    * **Uniforme:** $C = \log_2(\text{Salidas}) - H(\text{Fila})$.
* **An치lisis:**
    * Estas f칩rmulas solo son v치lidas bajo las condiciones estrictas de simetr칤a o determinismo del canal.
    * Permiten hallar $C$ sin necesidad de maximizar la Informaci칩n Mutua num칠ricamente.

### `estimarCapacidadCanalBinario(matriz, paso)`
* **Procedimiento:** Realiza un barrido de fuerza bruta ("grid search"). Prueba diferentes probabilidades a priori $P(A) = [p, 1-p]$ variando $p$ desde 0 a 1 con el `paso` dado. Calcula $I(A;B)$ para cada una y se queda con el m치ximo.
* **An치lisis:**
    * La **Capacidad ($C$)** se define como el m치ximo de la Informaci칩n Mutua sobre todas las posibles distribuciones de entrada.
    * Este m칠todo num칠rico es necesario cuando el canal no es sim칠trico ni uniforme (no hay f칩rmula cerrada simple).
    * La precisi칩n depende del tama침o del `paso`.

### `calcularProbabilidadError` ($P_E$ - Regla M치xima Posibilidad)
* **Procedimiento (Criterio C치tedra/Gu칤a):**
    1. Se define una **Regla de Decisi칩n Fija** basada 칰nicamente en la matriz del canal: para cada columna (salida), se elige la fila (entrada) con la probabilidad de transici칩n m치s alta ($P(b|a)$ m치ximo).
    2. Se calcula la probabilidad de error sumando las probabilidades $P(A, B)$ de todos los cruces que **no** fueron seleccionados por esa regla.
* **An치lisis:**
    * Eval칰a el desempe침o del canal asumiendo una decisi칩n basada en la "m치xima verosimilitud" (suponiendo a priori equiprobable para la decisi칩n, pero usando la a priori real para el c치lculo del error).
    * Un $P_E$ bajo indica que el canal conserva bien la identidad de los s칤mbolos.